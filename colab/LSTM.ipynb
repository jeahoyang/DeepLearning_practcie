{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGlDIzEaE9urg91XTYR0e9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeahoyang/DeepLearning_practcie/blob/main/colab/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0ZxZoLUV7MgW"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"\"\"\n",
        "ë‚˜ë¼ì˜ ë§ì´ ì¤‘êµ­ê³¼ ë‹¬ë¼ ë¬¸ìì™€ ì„œë¡œ í†µí•˜ì§€ ì•„ë‹ˆí•˜ë‹ˆ, ì´ëŸ° ê¹Œë‹­ìœ¼ë¡œ ì–´ë¦¬ì„ì€ ë°±ì„±ì´ ì´ë¥´ê³ ì í•  ë°”ê°€ ìˆì–´ë„ ë§ˆì¹¨ë‚´ ì œ ëœ»ì„ ëŠ¥íˆ í´ì§€ ëª»í•  ì‚¬ëŒì´ ë§ë‹¤. ë‚´ê°€ ì´ë¥¼ ìœ„í•˜ì—¬ ê°€ì—¾ì´ ì—¬ê²¨ ìƒˆë¡œ ìŠ¤ë¬¼ì—¬ëŸ ìë¥¼ ë§Œë“œë…¸ë‹ˆ ì‚¬ëŒë§ˆë‹¤ í•˜ì—¬ê¸ˆ ì‰¬ì´ ìµí˜€ ë‚ ë§ˆë‹¤ ì“°ëŠ” ë° í¸í•˜ê²Œ í•˜ê³ ì í•  ë”°ë¦„ì´ë‹¤.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hAxZii4Q7Qex"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocessing(data):\n",
        "  data = re.sub('[^ê°€-í£]', ' ', data)\n",
        "  tokens = data.split()\n",
        "  vocab = list(set(tokens))\n",
        "  vocab_size = len(vocab)\n",
        "\n",
        "  Word_to_ix = {Word: i for i, Word in enumerate(vocab)}\n",
        "  ix_to_Word = {i: Word for i, Word in enumerate(vocab)}\n",
        "\n",
        "  return tokens, vocab_size, Word_to_ix, ix_to_Word"
      ],
      "metadata": {
        "id": "2FIkulF17gXm"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activate function\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "  return x * (1 - x)\n",
        "\n",
        "def tanh(x, derivative=False):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "  return 1 - x ** 2\n",
        "\n",
        "def softmax(x):\n",
        "  return np.exp(x) / np.sum(np.exp(x))"
      ],
      "metadata": {
        "id": "EYN4roMWRBpB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM:\n",
        "  def __init__(self, input_size, hidden_size, output_size, num_epochs, learning_rate):\n",
        "\n",
        "    #Hyperparameters\n",
        "    self.learning_rate = learning_rate\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_epochs = num_epochs\n",
        "\n",
        "    # Forget Gate\n",
        "    self.Wf = np.random.randn(hidden_size, input_size)*0.1\n",
        "    self.bf = np.zeros((hidden_size, 1))\n",
        "\n",
        "    # Input Gate\n",
        "    self.Wi = np.random.randn(hidden_size, input_size)*0.1\n",
        "    self.bi = np.zeros((hidden_size, 1))\n",
        "\n",
        "    # Candidate Gate\n",
        "    self.Wc = np.random.randn(hidden_size, input_size)*0.1\n",
        "    self.bc = np.zeros((hidden_size, 1))\n",
        "\n",
        "    # Output Gate\n",
        "    self.Wo = np.random.randn(hidden_size, input_size)*0.1\n",
        "    self.bo = np.zeros((hidden_size, 1))\n",
        "\n",
        "    # Final Gate\n",
        "    self.Wy = np.random.randn(output_size, hidden_size)\n",
        "    self.by = np.zeros((output_size, 1))\n",
        "\n",
        "  # reset the network memory\n",
        "  def reset(self):\n",
        "    self.X = {}\n",
        "\n",
        "    self.HS = {-1: np.zeros((self.hidden_size, 1))}\n",
        "    self.CS = {-1: np.zeros((self.hidden_size, 1))}\n",
        "\n",
        "    self.C = {}\n",
        "    self.O = {}\n",
        "    self.F = {}\n",
        "    self.I = {}\n",
        "    self.outputs = {}\n",
        "\n",
        "  # Forward\n",
        "  def forward(self, inputs):\n",
        "    # self.reset()\n",
        "    x = {}\n",
        "    outputs = []\n",
        "    for t in range(len(inputs)):\n",
        "      x[t] = np.zeros((vocab_size, 1))\n",
        "      x[t][inputs[t]] = 1 # one hot encoding for each word\n",
        "      self.X[t] = np.concatenate((self.HS[t-1], x[t]))\n",
        "\n",
        "      self.F[t] = sigmoid(np.dot(self.Wf, self.X[t]) + self.bf)\n",
        "      self.I[t] = sigmoid(np.dot(self.Wi, self.X[t]) + self.bi)\n",
        "      self.C[t] = tanh(np.dot(self.Wc, self.X[t]) + self.bc)\n",
        "      self.O[t] = sigmoid(np.dot(self.Wo, self.X[t]) + self.bo)\n",
        "\n",
        "      self.CS[t] = self.F[t] * self.CS[t-1] + self.I[t] * self.C[t]\n",
        "      self.HS[t] = self.O[t] * tanh(self.CS[t])\n",
        "\n",
        "      outputs += [np.dot(self.Wy, self.HS[t]) + self.by]\n",
        "\n",
        "    return outputs\n",
        "\n",
        "  # Backward\n",
        "  def backward(self, errors, inputs):\n",
        "    dLdWf, dLdbf = 0, 0\n",
        "    dLdWi, dLdbi = 0, 0\n",
        "    dLdWc, dLdbc = 0, 0\n",
        "    dLdWo, dLdbo = 0, 0\n",
        "    dLdWy, dLdby = 0, 0\n",
        "\n",
        "    dh_next, dc_next = np.zeros_like(self.HS[0]), np.zeros_like(self.CS[0])\n",
        "\n",
        "    for t in reversed(range(len(inputs))):\n",
        "\n",
        "      error = errors[t]\n",
        "\n",
        "      # Final Gate Weights and Biases Errors\n",
        "      dLdWy += np.dot(error, self.HS[t].T)         #ğœ•ğ¿/ğœ•ğ‘Šğ‘¦\n",
        "      dLdby += error                               #ğœ•ğ¿/ğœ•bğ‘¦ = (ğœ•ğ¿/ğœ•z_t)(ğœ•z_t/ğœ•bğ‘¦) = error x 1 (Zt = WyHSt + by)\n",
        "\n",
        "      # Hidden State Error\n",
        "      dLdHS = np.dot(self.Wy.T, error) + dh_next    #ğœ•ğ¿/ğœ•ğ»ğ‘†\n",
        "\n",
        "      # Output Gate Weights and Biases Errors\n",
        "      dLdo = tanh(self.CS[t]) * dLdHS * sigmoid_derivative(self.O[t])\n",
        "      dLdWo += np.dot(dLdo, inputs[t].T)\n",
        "      dLdbo += dLdo\n",
        "\n",
        "      # Cell State Error\n",
        "      dLdCS = tanh_derivative(tanh(self.CS[t])) * self.O[t] * dLdHS + dc_next\n",
        "\n",
        "      # Forget Gate Weights and Biases Errors\n",
        "      dLdf = dLdCS * self.CS[t-1] * sigmoid_derivative(self.F[t])\n",
        "      dLdWf += np.dot(dLdf, inputs[t].T)\n",
        "      dLdbf += dLdf\n",
        "\n",
        "      # Input Gate Weights and Biases Errors\n",
        "      dLdI = dLdCS * self.C[t] * sigmoid_derivative(self.I[t])\n",
        "      dLdWi += np.dot(dLdI, inputs[t].T)\n",
        "      dLdbi += dLdI\n",
        "\n",
        "      # Candidate Gate Weights and Biases Errors\n",
        "      dLdC = dLdCS * self.I[t] * sigmoid_derivative(self.C[t])\n",
        "      dLdWc += np.dot(dLdC,inputs[t].T)\n",
        "      dLdbc += dLdC\n",
        "\n",
        "      # Concatenated Input Error (Sum of Error at Each Gate)\n",
        "      d_z = np.dot(self.Wf.T, dLdf) + np.dot(self.Wi.T, dLdI) + np.dot(self.Wc.T, dLdC) + np.dot(self.Wo.T, dLdo)\n",
        "\n",
        "      # Error of Hidden State and Cell State at Next Time Step\n",
        "      dh_next = d_z[:self.hidden_size, :]\n",
        "      dc_next = self.F[t] * dLdCS\n",
        "\n",
        "    for d in (dLdWf, dLdbf, dLdWi, dLdbi, dLdWc, dLdbc, dLdWo, dLdbo, dLdWy, dLdby):\n",
        "      np.clip(d, -1, 1, out=d)\n",
        "\n",
        "      self.Wf += dLdWf * self.learning_rate * (-1)\n",
        "      self.bf += dLdbf * self.learning_rate * (-1)\n",
        "\n",
        "      self.Wi += dLdWi * self.learning_rate * (-1)\n",
        "      self.bi += dLdbi * self.learning_rate * (-1)\n",
        "\n",
        "      self.Wc += dLdWc * self.learning_rate * (-1)\n",
        "      self.bc += dLdbc * self.learning_rate * (-1)\n",
        "\n",
        "      self.Wo += dLdWo * self.learning_rate * (-1)\n",
        "      self.bo += dLdbo * self.learning_rate * (-1)\n",
        "\n",
        "      self.Wy += dLdWy * self.learning_rate * (-1)\n",
        "      self.by += dLdby * self.learning_rate * (-1)\n",
        "\n",
        "    # Train\n",
        "  def train(self, inputs, labels):\n",
        "      for _ in tqdm(range(self.num_epochs)):\n",
        "          self.reset()\n",
        "          input_idx = [Word_to_ix[input] for input in inputs]\n",
        "          predictions = self.forward(input_idx)\n",
        "\n",
        "          errors = []\n",
        "          for t in range(len(predictions)):\n",
        "              errors += [softmax(predictions[t])]\n",
        "              errors[-1][Word_to_ix[labels[t]]] -= 1\n",
        "\n",
        "          self.backward(errors, self.X)\n",
        "\n",
        "  def test(self, inputs, labels):\n",
        "      accuracy = 0\n",
        "      probabilities = self.forward([Word_to_ix[input] for input in inputs])\n",
        "\n",
        "      gt = ''\n",
        "      output = 'ë‚˜ë¼ì˜ '\n",
        "      for q in range(len(labels)):\n",
        "          prediction = ix_to_Word[np.argmax(softmax(probabilities[q].reshape(-1)))]\n",
        "          gt += inputs[q] + ' '\n",
        "          output += prediction + ' '\n",
        "\n",
        "          if prediction == labels[q]:\n",
        "              accuracy += 1\n",
        "\n",
        "      print('\\n')\n",
        "      print('ì‹¤ì œê°’: ', gt)\n",
        "      print('ì˜ˆì¸¡ê°’: ', output)"
      ],
      "metadata": {
        "id": "qpDZv4IWRBk5"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 25\n",
        "\n",
        "# data preparation\n",
        "tokens, vocab_size, Word_to_ix, ix_to_Word = data_preprocessing(data)\n",
        "train_X, train_y = tokens[:-1], tokens[1:]\n",
        "\n",
        "\n",
        "lstm = LSTM(input_size=vocab_size + hidden_size, hidden_size=hidden_size, output_size=vocab_size, num_epochs=1000,\n",
        "            learning_rate=0.05)\n",
        "\n",
        "##### Training #####\n",
        "lstm.train(train_X, train_y)\n",
        "\n",
        "lstm.test(train_X, train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnPqzqlcRBi0",
        "outputId": "4c67bee7-00a4-45dc-ff42-00c0092a4a5c"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:06<00:00, 144.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ì‹¤ì œê°’:  ë‚˜ë¼ì˜ ë§ì´ ì¤‘êµ­ê³¼ ë‹¬ë¼ ë¬¸ìì™€ ì„œë¡œ í†µí•˜ì§€ ì•„ë‹ˆí•˜ë‹ˆ ì´ëŸ° ê¹Œë‹­ìœ¼ë¡œ ì–´ë¦¬ì„ì€ ë°±ì„±ì´ ì´ë¥´ê³ ì í•  ë°”ê°€ ìˆì–´ë„ ë§ˆì¹¨ë‚´ ì œ ëœ»ì„ ëŠ¥íˆ í´ì§€ ëª»í•  ì‚¬ëŒì´ ë§ë‹¤ ë‚´ê°€ ì´ë¥¼ ìœ„í•˜ì—¬ ê°€ì—¾ì´ ì—¬ê²¨ ìƒˆë¡œ ìŠ¤ë¬¼ì—¬ëŸ ìë¥¼ ë§Œë“œë…¸ë‹ˆ ì‚¬ëŒë§ˆë‹¤ í•˜ì—¬ê¸ˆ ì‰¬ì´ ìµí˜€ ë‚ ë§ˆë‹¤ ì“°ëŠ” ë° í¸í•˜ê²Œ í•˜ê³ ì í•  \n",
            "ì˜ˆì¸¡ê°’:  ë‚˜ë¼ì˜ í´ì§€ ì¤‘êµ­ê³¼ ë‹¬ë¼ ë¬¸ìì™€ ì„œë¡œ ìŠ¤ë¬¼ì—¬ëŸ ì•„ë‹ˆí•˜ë‹ˆ ì´ëŸ° ìˆì–´ë„ ë§Œë“œë…¸ë‹ˆ ìƒˆë¡œ ë§ë‹¤ í•  ë°”ê°€ ìë¥¼ ë§ì´ ì œ ëŠ¥íˆ ëŠ¥íˆ í•  ìœ„í•˜ì—¬ ìµí˜€ ë§ë‹¤ ì‚¬ëŒë§ˆë‹¤ ì´ë¥¼ ìœ„í•˜ì—¬ ë°±ì„±ì´ ëª»í•  ì‚¬ëŒì´ ë§ë‹¤ ìë¥¼ ë‚ ë§ˆë‹¤ ìµí˜€ í•˜ì—¬ê¸ˆ ë‚´ê°€ ìµí˜€ ë‚ ë§ˆë‹¤ ìµí˜€ ë° ìƒˆë¡œ í•˜ê³ ì í•  ë°”ê°€ \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YAUDNToMlmx1"
      },
      "execution_count": 88,
      "outputs": []
    }
  ]
}