{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGlDIzEaE9urg91XTYR0e9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeahoyang/DeepLearning_practcie/blob/main/colab/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0ZxZoLUV7MgW"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"\"\"\n",
        "나라의 말이 중국과 달라 문자와 서로 통하지 아니하니, 이런 까닭으로 어리석은 백성이 이르고자 할 바가 있어도 마침내 제 뜻을 능히 펴지 못할 사람이 많다. 내가 이를 위하여 가엾이 여겨 새로 스물여덟 자를 만드노니 사람마다 하여금 쉬이 익혀 날마다 쓰는 데 편하게 하고자 할 따름이다.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hAxZii4Q7Qex"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocessing(data):\n",
        "  data = re.sub('[^가-힣]', ' ', data)\n",
        "  tokens = data.split()\n",
        "  vocab = list(set(tokens))\n",
        "  vocab_size = len(vocab)\n",
        "\n",
        "  Word_to_ix = {Word: i for i, Word in enumerate(vocab)}\n",
        "  ix_to_Word = {i: Word for i, Word in enumerate(vocab)}\n",
        "\n",
        "  return tokens, vocab_size, Word_to_ix, ix_to_Word"
      ],
      "metadata": {
        "id": "2FIkulF17gXm"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activate function\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "  return x * (1 - x)\n",
        "\n",
        "def tanh(x, derivative=False):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "  return 1 - x ** 2\n",
        "\n",
        "def softmax(x):\n",
        "  return np.exp(x) / np.sum(np.exp(x))"
      ],
      "metadata": {
        "id": "EYN4roMWRBpB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM:\n",
        "  def __init__(self, input_size, hidden_size, output_size, num_epochs, learning_rate):\n",
        "\n",
        "    #Hyperparameters\n",
        "    self.learning_rate = learning_rate\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_epochs = num_epochs\n",
        "\n",
        "    # Forget Gate\n",
        "    self.Wf = np.random.randn(hidden_size, input_size)*0.1\n",
        "    self.bf = np.zeros((hidden_size, 1))\n",
        "\n",
        "    # Input Gate\n",
        "    self.Wi = np.random.randn(hidden_size, input_size)*0.1\n",
        "    self.bi = np.zeros((hidden_size, 1))\n",
        "\n",
        "    # Candidate Gate\n",
        "    self.Wc = np.random.randn(hidden_size, input_size)*0.1\n",
        "    self.bc = np.zeros((hidden_size, 1))\n",
        "\n",
        "    # Output Gate\n",
        "    self.Wo = np.random.randn(hidden_size, input_size)*0.1\n",
        "    self.bo = np.zeros((hidden_size, 1))\n",
        "\n",
        "    # Final Gate\n",
        "    self.Wy = np.random.randn(output_size, hidden_size)\n",
        "    self.by = np.zeros((output_size, 1))\n",
        "\n",
        "  # reset the network memory\n",
        "  def reset(self):\n",
        "    self.X = {}\n",
        "\n",
        "    self.HS = {-1: np.zeros((self.hidden_size, 1))}\n",
        "    self.CS = {-1: np.zeros((self.hidden_size, 1))}\n",
        "\n",
        "    self.C = {}\n",
        "    self.O = {}\n",
        "    self.F = {}\n",
        "    self.I = {}\n",
        "    self.outputs = {}\n",
        "\n",
        "  # Forward\n",
        "  def forward(self, inputs):\n",
        "    # self.reset()\n",
        "    x = {}\n",
        "    outputs = []\n",
        "    for t in range(len(inputs)):\n",
        "      x[t] = np.zeros((vocab_size, 1))\n",
        "      x[t][inputs[t]] = 1 # one hot encoding for each word\n",
        "      self.X[t] = np.concatenate((self.HS[t-1], x[t]))\n",
        "\n",
        "      self.F[t] = sigmoid(np.dot(self.Wf, self.X[t]) + self.bf)\n",
        "      self.I[t] = sigmoid(np.dot(self.Wi, self.X[t]) + self.bi)\n",
        "      self.C[t] = tanh(np.dot(self.Wc, self.X[t]) + self.bc)\n",
        "      self.O[t] = sigmoid(np.dot(self.Wo, self.X[t]) + self.bo)\n",
        "\n",
        "      self.CS[t] = self.F[t] * self.CS[t-1] + self.I[t] * self.C[t]\n",
        "      self.HS[t] = self.O[t] * tanh(self.CS[t])\n",
        "\n",
        "      outputs += [np.dot(self.Wy, self.HS[t]) + self.by]\n",
        "\n",
        "    return outputs\n",
        "\n",
        "  # Backward\n",
        "  def backward(self, errors, inputs):\n",
        "    dLdWf, dLdbf = 0, 0\n",
        "    dLdWi, dLdbi = 0, 0\n",
        "    dLdWc, dLdbc = 0, 0\n",
        "    dLdWo, dLdbo = 0, 0\n",
        "    dLdWy, dLdby = 0, 0\n",
        "\n",
        "    dh_next, dc_next = np.zeros_like(self.HS[0]), np.zeros_like(self.CS[0])\n",
        "\n",
        "    for t in reversed(range(len(inputs))):\n",
        "\n",
        "      error = errors[t]\n",
        "\n",
        "      # Final Gate Weights and Biases Errors\n",
        "      dLdWy += np.dot(error, self.HS[t].T)         #𝜕𝐿/𝜕𝑊𝑦\n",
        "      dLdby += error                               #𝜕𝐿/𝜕b𝑦 = (𝜕𝐿/𝜕z_t)(𝜕z_t/𝜕b𝑦) = error x 1 (Zt = WyHSt + by)\n",
        "\n",
        "      # Hidden State Error\n",
        "      dLdHS = np.dot(self.Wy.T, error) + dh_next    #𝜕𝐿/𝜕𝐻𝑆\n",
        "\n",
        "      # Output Gate Weights and Biases Errors\n",
        "      dLdo = tanh(self.CS[t]) * dLdHS * sigmoid_derivative(self.O[t])\n",
        "      dLdWo += np.dot(dLdo, inputs[t].T)\n",
        "      dLdbo += dLdo\n",
        "\n",
        "      # Cell State Error\n",
        "      dLdCS = tanh_derivative(tanh(self.CS[t])) * self.O[t] * dLdHS + dc_next\n",
        "\n",
        "      # Forget Gate Weights and Biases Errors\n",
        "      dLdf = dLdCS * self.CS[t-1] * sigmoid_derivative(self.F[t])\n",
        "      dLdWf += np.dot(dLdf, inputs[t].T)\n",
        "      dLdbf += dLdf\n",
        "\n",
        "      # Input Gate Weights and Biases Errors\n",
        "      dLdI = dLdCS * self.C[t] * sigmoid_derivative(self.I[t])\n",
        "      dLdWi += np.dot(dLdI, inputs[t].T)\n",
        "      dLdbi += dLdI\n",
        "\n",
        "      # Candidate Gate Weights and Biases Errors\n",
        "      dLdC = dLdCS * self.I[t] * sigmoid_derivative(self.C[t])\n",
        "      dLdWc += np.dot(dLdC,inputs[t].T)\n",
        "      dLdbc += dLdC\n",
        "\n",
        "      # Concatenated Input Error (Sum of Error at Each Gate)\n",
        "      d_z = np.dot(self.Wf.T, dLdf) + np.dot(self.Wi.T, dLdI) + np.dot(self.Wc.T, dLdC) + np.dot(self.Wo.T, dLdo)\n",
        "\n",
        "      # Error of Hidden State and Cell State at Next Time Step\n",
        "      dh_next = d_z[:self.hidden_size, :]\n",
        "      dc_next = self.F[t] * dLdCS\n",
        "\n",
        "    for d in (dLdWf, dLdbf, dLdWi, dLdbi, dLdWc, dLdbc, dLdWo, dLdbo, dLdWy, dLdby):\n",
        "      np.clip(d, -1, 1, out=d)\n",
        "\n",
        "      self.Wf += dLdWf * self.learning_rate * (-1)\n",
        "      self.bf += dLdbf * self.learning_rate * (-1)\n",
        "\n",
        "      self.Wi += dLdWi * self.learning_rate * (-1)\n",
        "      self.bi += dLdbi * self.learning_rate * (-1)\n",
        "\n",
        "      self.Wc += dLdWc * self.learning_rate * (-1)\n",
        "      self.bc += dLdbc * self.learning_rate * (-1)\n",
        "\n",
        "      self.Wo += dLdWo * self.learning_rate * (-1)\n",
        "      self.bo += dLdbo * self.learning_rate * (-1)\n",
        "\n",
        "      self.Wy += dLdWy * self.learning_rate * (-1)\n",
        "      self.by += dLdby * self.learning_rate * (-1)\n",
        "\n",
        "    # Train\n",
        "  def train(self, inputs, labels):\n",
        "      for _ in tqdm(range(self.num_epochs)):\n",
        "          self.reset()\n",
        "          input_idx = [Word_to_ix[input] for input in inputs]\n",
        "          predictions = self.forward(input_idx)\n",
        "\n",
        "          errors = []\n",
        "          for t in range(len(predictions)):\n",
        "              errors += [softmax(predictions[t])]\n",
        "              errors[-1][Word_to_ix[labels[t]]] -= 1\n",
        "\n",
        "          self.backward(errors, self.X)\n",
        "\n",
        "  def test(self, inputs, labels):\n",
        "      accuracy = 0\n",
        "      probabilities = self.forward([Word_to_ix[input] for input in inputs])\n",
        "\n",
        "      gt = ''\n",
        "      output = '나라의 '\n",
        "      for q in range(len(labels)):\n",
        "          prediction = ix_to_Word[np.argmax(softmax(probabilities[q].reshape(-1)))]\n",
        "          gt += inputs[q] + ' '\n",
        "          output += prediction + ' '\n",
        "\n",
        "          if prediction == labels[q]:\n",
        "              accuracy += 1\n",
        "\n",
        "      print('\\n')\n",
        "      print('실제값: ', gt)\n",
        "      print('예측값: ', output)"
      ],
      "metadata": {
        "id": "qpDZv4IWRBk5"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 25\n",
        "\n",
        "# data preparation\n",
        "tokens, vocab_size, Word_to_ix, ix_to_Word = data_preprocessing(data)\n",
        "train_X, train_y = tokens[:-1], tokens[1:]\n",
        "\n",
        "\n",
        "lstm = LSTM(input_size=vocab_size + hidden_size, hidden_size=hidden_size, output_size=vocab_size, num_epochs=1000,\n",
        "            learning_rate=0.05)\n",
        "\n",
        "##### Training #####\n",
        "lstm.train(train_X, train_y)\n",
        "\n",
        "lstm.test(train_X, train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnPqzqlcRBi0",
        "outputId": "4c67bee7-00a4-45dc-ff42-00c0092a4a5c"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:06<00:00, 144.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "실제값:  나라의 말이 중국과 달라 문자와 서로 통하지 아니하니 이런 까닭으로 어리석은 백성이 이르고자 할 바가 있어도 마침내 제 뜻을 능히 펴지 못할 사람이 많다 내가 이를 위하여 가엾이 여겨 새로 스물여덟 자를 만드노니 사람마다 하여금 쉬이 익혀 날마다 쓰는 데 편하게 하고자 할 \n",
            "예측값:  나라의 펴지 중국과 달라 문자와 서로 스물여덟 아니하니 이런 있어도 만드노니 새로 많다 할 바가 자를 말이 제 능히 능히 할 위하여 익혀 많다 사람마다 이를 위하여 백성이 못할 사람이 많다 자를 날마다 익혀 하여금 내가 익혀 날마다 익혀 데 새로 하고자 할 바가 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YAUDNToMlmx1"
      },
      "execution_count": 88,
      "outputs": []
    }
  ]
}