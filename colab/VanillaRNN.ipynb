{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJtCi24+/xF2+17W226QkH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeahoyang/DeepLearning_practcie/blob/main/colab/VanillaRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0ZxZoLUV7MgW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"\"\"\n",
        "나라의 말이 중국과 달라 문자와 서로 통하지 아니하니, 이런 까닭으로 어리석은 백성이 이르고자 할 바가 있어도 마침내 제 뜻을 능히 펴지 못할 사람이 많다. 내가 이를 위하여 가엾이 여겨 새로 스물여덟 자를 만드노니 사람마다 하여금 쉬이 익혀 날마다 쓰는 데 편하게 하고자 할 따름이다.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hAxZii4Q7Qex"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocessing(data):\n",
        "  data = re.sub('[^가-힣]', ' ', data)\n",
        "  tokens = data.split()\n",
        "  vocab = list(set(tokens))\n",
        "  vocab_size = len(vocab)\n",
        "\n",
        "  word2ix = {word: i for i, word in enumerate(vocab)}\n",
        "  ix2word = {i: word for i, word in enumerate(vocab)}\n",
        "\n",
        "  return tokens, vocab_size, word2ix, ix2word"
      ],
      "metadata": {
        "id": "2FIkulF17gXm"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(h_size, vocab_size):\n",
        "  U = np.random.randn(h_size, vocab_size) * 0.01\n",
        "  W = np.random.randn(h_size, h_size) * 0.01\n",
        "  V = np.random.randn(vocab_size, h_size) * 0.01\n",
        "  return U,W,V"
      ],
      "metadata": {
        "id": "9Cz5XVBh8JGE"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feedforward(inputs, targets, hprev):\n",
        "  loss = 0\n",
        "  xs, hs, ps, ys = {}, {}, {}, {}\n",
        "  hs[-1] = np.copy(hprev)\n",
        "  for i in range(seq_len):\n",
        "    xs[i] = np.zeros((vocab_size, 1))\n",
        "    xs[i][inputs[i]] = 1 # one hot encoding for each word\n",
        "    hs[i] = np.tanh(np.dot(U, xs[i]) + np.dot(W, hs[i-1]))\n",
        "    ys[i] = np.dot(V, hs[i])\n",
        "    ps[i] = np.exp(ys[i]) / np.sum(np.exp(ys[i])) # softmax calculate\n",
        "    loss += -np.log(ps[i][targets[i], 0]) # cross entropy\n",
        "\n",
        "  return loss, ps, hs, xs"
      ],
      "metadata": {
        "id": "3EUwUE2w9LsV"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward(ps, hs, xs):\n",
        "\n",
        "  # Backwaard propagation through time (BPTT)\n",
        "  # Initial bias set to 0\n",
        "  dV = np.zeros(V.shape)\n",
        "  dW = np.zeros(W.shape)\n",
        "  dU = np.zeros(U.shape)\n",
        "\n",
        "  for i in range(seq_len)[::-1]: # This is because to go in reverse order\n",
        "    output = np.zeros((vocab_size, 1))\n",
        "    output[targets[i]] = 1\n",
        "    ps[i] = ps[i] - output.reshape(-1,1)\n",
        "    dV_step_i = ps[i] @ (hs[i]).T # (y_hat - y) @ hs.T  for each step\n",
        "\n",
        "    dV = dV + dV_step_i # sum all of dL/dVi\n",
        "\n",
        "    # To find V and W for each i,\n",
        "    # first calculate the commonly calculated part as delta,\n",
        "    # then go back in time to find dL/dWij and dL/dUij,\n",
        "    # add them together to get dL/dW and dL/dU,\n",
        "    # and update the commonly calculated delta again.\n",
        "\n",
        "    # delta to be commonly used in the ith step\n",
        "    delta_recent = (V.T @ ps[i]) * (1 - hs[i] ** 2)\n",
        "\n",
        "    # Go back in time and find dL/dW and dL/dU\n",
        "    for j in range(i+1)[::-1]:\n",
        "      dW_ij = delta_recent @ hs[j - 1].T\n",
        "      dW = dW + dW_ij\n",
        "\n",
        "      dU_ij = delta_recent @ xs[j].reshape(1, -1)\n",
        "      dU = dU + dU_ij\n",
        "\n",
        "      # Delta update to be commonly calculated in the next step j\n",
        "      delta_recent = (W.T @ delta_recent) * (1 - hs[j - 1] ** 2)\n",
        "\n",
        "      for d in [dV, dW, dU]:\n",
        "        np.clip(d, -1, 1, out=d)\n",
        "\n",
        "  return dV, dW, dU, hs[len(inputs) -1]"
      ],
      "metadata": {
        "id": "1d-2meXq-Whs"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(word, length):\n",
        "  x = np.zeros((vocab_size, 1))\n",
        "  x[word2ix[word]] = 1\n",
        "  ixes = []\n",
        "  h = np.zeros((h_size, 1))\n",
        "\n",
        "  for t in range(length):\n",
        "    h = np.tanh(np.dot(U, x) + np.dot(W, h))\n",
        "    y = np.dot(V, h)\n",
        "    p = np.exp(y) / np.sum(np.exp(y)) # softmax\n",
        "    ix = np.argmax(p)                 # Return the index with the highest probability\n",
        "    x = np.zeros((vocab_size, 1))     # prepare next input x\n",
        "    x[ix] = 1\n",
        "    ixes.append(ix)\n",
        "\n",
        "  pred_words = ' '.join(ix2word[i] for i in ixes)\n",
        "\n",
        "  return pred_words"
      ],
      "metadata": {
        "id": "KiVDdEwmB9uS"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "\n",
        "epochs = 10000\n",
        "h_size = 100\n",
        "seq_len = 3\n",
        "learning_rate = 1e-2"
      ],
      "metadata": {
        "id": "9o68rt5tDLmO"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens, vocab_size, word2ix, ix2word = data_preprocessing(data)"
      ],
      "metadata": {
        "id": "DVeHOtcAD5Oc"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7fAABvnD_qT",
        "outputId": "303054e4-a69a-411e-c768-35913dcecf8b"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['나라의',\n",
              " '말이',\n",
              " '중국과',\n",
              " '달라',\n",
              " '문자와',\n",
              " '서로',\n",
              " '통하지',\n",
              " '아니하니',\n",
              " '이런',\n",
              " '까닭으로',\n",
              " '어리석은',\n",
              " '백성이',\n",
              " '이르고자',\n",
              " '할',\n",
              " '바가',\n",
              " '있어도',\n",
              " '마침내',\n",
              " '제',\n",
              " '뜻을',\n",
              " '능히',\n",
              " '펴지',\n",
              " '못할',\n",
              " '사람이',\n",
              " '많다',\n",
              " '내가',\n",
              " '이를',\n",
              " '위하여',\n",
              " '가엾이',\n",
              " '여겨',\n",
              " '새로',\n",
              " '스물여덟',\n",
              " '자를',\n",
              " '만드노니',\n",
              " '사람마다',\n",
              " '하여금',\n",
              " '쉬이',\n",
              " '익혀',\n",
              " '날마다',\n",
              " '쓰는',\n",
              " '데',\n",
              " '편하게',\n",
              " '하고자',\n",
              " '할',\n",
              " '따름이다']"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2WUSs4nEBCo",
        "outputId": "6b1a762d-5643-41a8-a82f-de764a86b150"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2ix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKrTIWKDEE_G",
        "outputId": "c46589de-513c-49b3-f9bb-6f2392c0417b"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'바가': 0,\n",
              " '하여금': 1,\n",
              " '까닭으로': 2,\n",
              " '하고자': 3,\n",
              " '어리석은': 4,\n",
              " '사람이': 5,\n",
              " '쓰는': 6,\n",
              " '스물여덟': 7,\n",
              " '백성이': 8,\n",
              " '만드노니': 9,\n",
              " '가엾이': 10,\n",
              " '익혀': 11,\n",
              " '편하게': 12,\n",
              " '많다': 13,\n",
              " '제': 14,\n",
              " '문자와': 15,\n",
              " '중국과': 16,\n",
              " '내가': 17,\n",
              " '서로': 18,\n",
              " '날마다': 19,\n",
              " '이를': 20,\n",
              " '여겨': 21,\n",
              " '아니하니': 22,\n",
              " '있어도': 23,\n",
              " '쉬이': 24,\n",
              " '데': 25,\n",
              " '통하지': 26,\n",
              " '못할': 27,\n",
              " '이런': 28,\n",
              " '자를': 29,\n",
              " '나라의': 30,\n",
              " '마침내': 31,\n",
              " '따름이다': 32,\n",
              " '펴지': 33,\n",
              " '능히': 34,\n",
              " '말이': 35,\n",
              " '달라': 36,\n",
              " '할': 37,\n",
              " '위하여': 38,\n",
              " '새로': 39,\n",
              " '사람마다': 40,\n",
              " '뜻을': 41,\n",
              " '이르고자': 42}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ix2word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6EWhvI-EMbM",
        "outputId": "e988aa34-5ca2-4a91-d581-9371c09e9a50"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '바가',\n",
              " 1: '하여금',\n",
              " 2: '까닭으로',\n",
              " 3: '하고자',\n",
              " 4: '어리석은',\n",
              " 5: '사람이',\n",
              " 6: '쓰는',\n",
              " 7: '스물여덟',\n",
              " 8: '백성이',\n",
              " 9: '만드노니',\n",
              " 10: '가엾이',\n",
              " 11: '익혀',\n",
              " 12: '편하게',\n",
              " 13: '많다',\n",
              " 14: '제',\n",
              " 15: '문자와',\n",
              " 16: '중국과',\n",
              " 17: '내가',\n",
              " 18: '서로',\n",
              " 19: '날마다',\n",
              " 20: '이를',\n",
              " 21: '여겨',\n",
              " 22: '아니하니',\n",
              " 23: '있어도',\n",
              " 24: '쉬이',\n",
              " 25: '데',\n",
              " 26: '통하지',\n",
              " 27: '못할',\n",
              " 28: '이런',\n",
              " 29: '자를',\n",
              " 30: '나라의',\n",
              " 31: '마침내',\n",
              " 32: '따름이다',\n",
              " 33: '펴지',\n",
              " 34: '능히',\n",
              " 35: '말이',\n",
              " 36: '달라',\n",
              " 37: '할',\n",
              " 38: '위하여',\n",
              " 39: '새로',\n",
              " 40: '사람마다',\n",
              " 41: '뜻을',\n",
              " 42: '이르고자'}"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "U, W, V = init_weights(h_size, vocab_size)"
      ],
      "metadata": {
        "id": "7tVzFy2KER6P"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = 0\n",
        "hprev = np.zeros((h_size, 1))\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  for p in range(len(tokens)-seq_len):\n",
        "    inputs = [word2ix[word] for word in tokens[p:p + seq_len]]\n",
        "    targets = [word2ix[word] for word in tokens[p + 1:p + seq_len + 1]]\n",
        "\n",
        "    loss, ps, hs, xs = feedforward(inputs, targets, hprev)\n",
        "    dV, dW, dU, hprev = backward(ps, hs, xs)\n",
        "\n",
        "    # Update weights and biases using gradient descent\n",
        "    W -= learning_rate * dW\n",
        "    U -= learning_rate * dU\n",
        "    V -= learning_rate * dV\n",
        "\n",
        "    # p += seq_len\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(f'epoch: {epoch}, loss: {loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WN0A-sLDEaXJ",
        "outputId": "ea496d61-6881-47cd-a0d1-5f741ab29e50"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, loss: 0.16826204103608933\n",
            "epoch: 100, loss: 0.045075354562351205\n",
            "epoch: 200, loss: 0.02719595139490123\n",
            "epoch: 300, loss: 0.020454189133170063\n",
            "epoch: 400, loss: 0.01680663150625597\n",
            "epoch: 500, loss: 0.014438260748168024\n",
            "epoch: 600, loss: 0.012737071948426326\n",
            "epoch: 700, loss: 0.01143433470452837\n",
            "epoch: 800, loss: 0.010391864227331666\n",
            "epoch: 900, loss: 0.009531135746568876\n",
            "epoch: 1000, loss: 0.008804103547274865\n",
            "epoch: 1100, loss: 0.008179548716693729\n",
            "epoch: 1200, loss: 0.00763613833886128\n",
            "epoch: 1300, loss: 0.007158653728040396\n",
            "epoch: 1400, loss: 0.0067358183168768435\n",
            "epoch: 1500, loss: 0.006358987639106922\n",
            "epoch: 1600, loss: 0.006021334450523641\n",
            "epoch: 1700, loss: 0.005717332178962401\n",
            "epoch: 1800, loss: 0.005442421900738794\n",
            "epoch: 1900, loss: 0.005192791947861225\n",
            "epoch: 2000, loss: 0.004965226018936524\n",
            "epoch: 2100, loss: 0.00475699313049654\n",
            "epoch: 2200, loss: 0.004565763935241503\n",
            "epoch: 2300, loss: 0.004389544646858366\n",
            "epoch: 2400, loss: 0.004226623546626689\n",
            "epoch: 2500, loss: 0.004075527010723732\n",
            "epoch: 2600, loss: 0.003934983019417646\n",
            "epoch: 2700, loss: 0.0038038906770203424\n",
            "epoch: 2800, loss: 0.0036812946283440668\n",
            "epoch: 2900, loss: 0.003566363507834935\n",
            "epoch: 3000, loss: 0.0034583717417668303\n",
            "epoch: 3100, loss: 0.003356684159355467\n",
            "epoch: 3200, loss: 0.003260742967625037\n",
            "epoch: 3300, loss: 0.0031700567184373904\n",
            "epoch: 3400, loss: 0.003084190953559251\n",
            "epoch: 3500, loss: 0.0030027602614892397\n",
            "epoch: 3600, loss: 0.002925421521428501\n",
            "epoch: 3700, loss: 0.002851868146813414\n",
            "epoch: 3800, loss: 0.0027818251732224146\n",
            "epoch: 3900, loss: 0.002715045063167593\n",
            "epoch: 4000, loss: 0.002651304123130029\n",
            "epoch: 4100, loss: 0.00259039944634362\n",
            "epoch: 4200, loss: 0.002532146308847131\n",
            "epoch: 4300, loss: 0.002476375956824436\n",
            "epoch: 4400, loss: 0.0024229337311096532\n",
            "epoch: 4500, loss: 0.002371677480658697\n",
            "epoch: 4600, loss: 0.002322476221549308\n",
            "epoch: 4700, loss: 0.002275209002263483\n",
            "epoch: 4800, loss: 0.002229763939952853\n",
            "epoch: 4900, loss: 0.0021860373964302663\n",
            "epoch: 5000, loss: 0.002143933266775988\n",
            "epoch: 5100, loss: 0.002103362357680443\n",
            "epoch: 5200, loss: 0.002064241836816052\n",
            "epoch: 5300, loss: 0.002026494738442277\n",
            "epoch: 5400, loss: 0.0019900495140190056\n",
            "epoch: 5500, loss: 0.001954839619589801\n",
            "epoch: 5600, loss: 0.001920803134151711\n",
            "epoch: 5700, loss: 0.0018878824050786985\n",
            "epoch: 5800, loss: 0.0018560237179902224\n",
            "epoch: 5900, loss: 0.0018251769893267125\n",
            "epoch: 6000, loss: 0.001795295480403096\n",
            "epoch: 6100, loss: 0.0017663355319790617\n",
            "epoch: 6200, loss: 0.001738256318470903\n",
            "epoch: 6300, loss: 0.0017110196209202148\n",
            "epoch: 6400, loss: 0.0016845896177817095\n",
            "epoch: 6500, loss: 0.0016589326925406916\n",
            "epoch: 6600, loss: 0.0016340172571183875\n",
            "epoch: 6700, loss: 0.0016098135899816081\n",
            "epoch: 6800, loss: 0.0015862936879043042\n",
            "epoch: 6900, loss: 0.0015634311303260127\n",
            "epoch: 7000, loss: 0.0015412009553073085\n",
            "epoch: 7100, loss: 0.001519579546132156\n",
            "epoch: 7200, loss: 0.0014985445276759239\n",
            "epoch: 7300, loss: 0.0014780746717151971\n",
            "epoch: 7400, loss: 0.0014581498104368996\n",
            "epoch: 7500, loss: 0.0014387507574578056\n",
            "epoch: 7600, loss: 0.0014198592357220732\n",
            "epoch: 7700, loss: 0.0014014578117178644\n",
            "epoch: 7800, loss: 0.001383529835471534\n",
            "epoch: 7900, loss: 0.001366059385864501\n",
            "epoch: 8000, loss: 0.0013490312208079868\n",
            "epoch: 8100, loss: 0.0013324307318886334\n",
            "epoch: 8200, loss: 0.0013162439030987455\n",
            "epoch: 8300, loss: 0.0013004572733099362\n",
            "epoch: 8400, loss: 0.001285057902155678\n",
            "epoch: 8500, loss: 0.00127003333903839\n",
            "epoch: 8600, loss: 0.0012553715949640812\n",
            "epoch: 8700, loss: 0.0012410611169550202\n",
            "epoch: 8800, loss: 0.001227090764792168\n",
            "epoch: 8900, loss: 0.0012134497898677828\n",
            "epoch: 9000, loss: 0.0012001278159337533\n",
            "epoch: 9100, loss: 0.0011871148215557691\n",
            "epoch: 9200, loss: 0.001174401124104801\n",
            "epoch: 9300, loss: 0.0011619773651025914\n",
            "epoch: 9400, loss: 0.0011498344968050816\n",
            "epoch: 9500, loss: 0.0011379637698510595\n",
            "epoch: 9600, loss: 0.001126356721883302\n",
            "epoch: 9700, loss: 0.0011150051670223925\n",
            "epoch: 9800, loss: 0.0011039011860825053\n",
            "epoch: 9900, loss: 0.0010930371174596785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while 1:\n",
        "  try:\n",
        "    user_input = input(\"input: \")\n",
        "    if user_input == 'break':\n",
        "      break\n",
        "    response = predict(user_input, 20)\n",
        "    print(response)\n",
        "\n",
        "  except:\n",
        "    print('try again!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb1AD2OQF0oE",
        "outputId": "043e9216-9d84-4409-c5f7-30e166b9f216"
      },
      "execution_count": 76,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "말이 중국과 달라 문자와 달라 문자와 달라 문자와 서로 통하지 서로 통하지 서로 통하지 아니하니 통하지 아니하니 이런 아니하니 이런\n",
            "input: break\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iYa6I3qZKxy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SHHVg2G8KwSS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}